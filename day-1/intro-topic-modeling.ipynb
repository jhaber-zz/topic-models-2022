{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Topic Modeling\n",
    "## Day 1: The Basics\n",
    "---\n",
    "---\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "* Think through how and why you might use topic modeling in a text analysis project\n",
    "* Implement a basic topic modeling algorithm and learn how to tweak it\n",
    "* Understand the intuitions behind `Latent Dirichlet Allocation` (LDA) for topic modeling\n",
    "* Get familiar with (some of) Python's versatile `scikit-learn` library\n",
    "* Get familiar with the `Pandas` package and manipulating data\n",
    "* Learn how to prepare a `Document-Term Matrix` (DTM), why this is useful, and what \"bag of words\" means\n",
    "\n",
    "\n",
    "## Outline\n",
    "- [The 'what' and 'why' of topic modeling](#whatwhy)\n",
    "    - [How LDA works (briefly)](#mechanics)\n",
    "- [How to join in with coding](#joinin)\n",
    "- [The Pandas dataframe: Children's literature](#data)\n",
    "    - [Exploratory data analysis](#explore)\n",
    "- [Text preprocessing](#preprocess)\n",
    "- [Train a topic model with LDA](#train)\n",
    "- [Document-by-topic distribution](#topics)\n",
    "- [LDA as dimensionality reduction](#dimensions)\n",
    "\n",
    "\n",
    "## Key terms\n",
    "* *document-term matrix (DTM)*:\n",
    "    * a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a DTM, rows correspond to documents in the collection and columns correspond to terms. Making a DTM is a common preprocessing step: it takes in tokenized texts and makes them ready for downstream tasks (e.g., topic models, supervised machine learning models).\n",
    "* *topic modeling*:\n",
    "    * A statistical model to uncover abstract topics within a text. It uses the co-occurrence fo words within documents, compared to their distribution across documents, to uncover these abstract themes. The output is a list of weighted words, which indicate the subject of each topic, and a weight distribution across topics for each document.\n",
    "* *Latent Dirichlet Allocation (LDA)*:\n",
    "    * A popular implementation of topic modeling that assumes a Dirichlet prior. It estimates a distribution of documents over topics and a distribution of words over topics. It does not take document order into account (unlike some others topic modeling algorithms).\n",
    "* *unsupervised text analysis*:\n",
    "    * Learning patterns or structure in text data through the associations of linguistic units (e.g., word co-associations). Uses include discovering latent clusters or dimensions, reducing dimensionality, outlier detection, and probability density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 'what' and 'why' of topic modeling <a id='whatwhy'></a>\n",
    "\n",
    "Topic modeling is a _distant reading_ technique for finding structure in large collections of text, without actually reading everything by eye. If you have hundreds or thousands of documents and want to understand roughly what your corpus contains, then topic modeling may be for you.\n",
    "\n",
    "A topic modeling program finds the words that appear frequently together in a document and groups them together to form **topics**, which are mixtures of words that characterize the document's themes or underlying ideas. Topic modeling is a kind of _unsupervised machine learning_ that learns what topics characterize a corpus based on word co-associations within and across documents.\n",
    "\n",
    "Topic modeling may be used to support different approaches to large text corpora, such as:\n",
    "\n",
    "* Survey a collection that is too big to read closely, e.g. [Computational Historiography: Data Mining in a Century of Classics Journals](http://www.perseus.tufts.edu/publications/02-jocch-mimno.pdf) (PDF)\n",
    "* Look at thematic trends over time in an archive, e.g. [Topic Modeling Martha Ballard's Diary](http://www.cameronblevins.org/posts/topic-modeling-martha-ballards-diary/)\n",
    "* Create metadata for an archive to improve accessibility, e.g. [Topic modeling for the valorisation of digitised archives of the European Commission](https://ieeexplore.ieee.org/abstract/document/7840981)\n",
    "* Understand current trends in social media relevant to your discipline, e.g. [Mining the Open Web with ‘Looted Heritage’](https://electricarchaeology.ca/2012/06/08/mining-the-open-web-with-looted-heritage-draft/)\n",
    "\n",
    "As a simple example, one topic of this [Wikipedia article on black holes](https://en.wikipedia.org/wiki/Black_hole) is:\n",
    "\n",
    "* black, hole, mass, star\n",
    "\n",
    "![First picture of a supermassive black hole, captured in 2019](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Black_hole_-_Messier_87.jpg/320px-Black_hole_-_Messier_87.jpg \"First picture of a supermassive black hole, captured in 2019\")\n",
    "\n",
    "Not too surprising, but this topic does seem accurate on the face of it. What about a document that we are less familiar with? Here is a topic of a [speech made by John F. Kennedy at Rice University in 1962](https://er.jsc.nasa.gov/seh/ricetalk.htm):\n",
    "\n",
    "* space, new, year, man\n",
    "\n",
    "![Charles Conrad Jr., Apollo 12 Commander, examines the unmanned Surveyor III spacecraft on the Moon](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Surveyor_3-Apollo_12.jpg/274px-Surveyor_3-Apollo_12.jpg \"Charles Conrad Jr., Apollo 12 Commander, examines the unmanned Surveyor III spacecraft on the Moon\")\n",
    "\n",
    "This is Kennedy's famous 'we choose to go to the moon' speech. Notice that 'moon' is not in this topic; but the speech does cover the history of humankind's (\"man's\") endeavours and emphasises a forward-looking perspective (the \"new\"-ness of advancements).\n",
    "\n",
    "As suggested by the simplified examples above, human intervention is still required to interpret what topics might 'mean'. Topic modeling is not magic; it is a tool that requires informed use and careful review, just like any other. Ideally, when interpreting topics we consider topics' frequent and distinctive words as well as specific cases with interesting topic loadings (i.e., example texts high in topics of interest). But just a few frequent words by topic can tell us a lot&mdash;such as this raw model output for a topic model I built on charter school websites:\n",
    "\n",
    "<img src=\"../assets/topic_prevalence.png\" alt=\"Topic model output\" width=\"600\" height=\"300\"/>\n",
    "\n",
    "I name the top topic \"standards & assessment\", the second \"communication\", the third \"web administration\", etc. What would you call the topic fourth from the top? The bottom one? \n",
    "\n",
    "What use is such a model? In addition to describing what topics/ideas are expressed in a text corpus, we can consider these topics' many contexts: race and class, emotions, popularity, etc. This lets us answer questions like: Which topics are less common in white charter school websites? These regression results help us answer that particular question:\n",
    "\n",
    "<img src=\"../assets/topics_and_race.png\" alt=\"Topics and race\" width=\"600\" height=\"300\"/>\n",
    "\n",
    "We can also look at how topics change over time, producing visuals like this one, which visualizes the topics of Trump's tweets in 2020.\n",
    "\n",
    "<img src=\"../assets/trump_topics.png\" alt=\"Trump's most frequent topics\" width=\"1000\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How LDA works (briefly) <a id='mechanics'></a>\n",
    "\n",
    "We'll implement the original and simplest topic modeling algorithm, Latent Dirichlet Allocation (or LDA), using Python's scikit-learn. There is a lot to learn about topic modeling, but the focus here is giving you starter code you can expand on later. To give a little more mathematical context, though, here are the mechanics of this version of topic modeling. \n",
    "\n",
    "LDA is a generative model&mdash;a model of the data-generating process&mdash;in which documents are mixtures of topics and topics are probability distributions over tokens in the vocabulary.$^1$ The (normalized) frequency of word $j$ in document $i$ can be written as:\n",
    "\n",
    "$q_{ij} = v_{i1}*\\theta_{1j} + v_{i2}*\\theta_{2j} + ... + v_{iK}*\\theta_{Kj}$\n",
    "\n",
    "where K is the total number of topics, $\\theta_{kj}$ is the probability that word $j$ shows up in topic $k$ and $v_{ik}$ is the weight assigned to topic $k$ in document $i$. The model treats $v$ and $\\theta$ as generated from Dirichlet-distributed priors and estimates them with Maximum Likelihood or Bayesian methods.\n",
    "\n",
    "These mechanics can be illustrated as follows:$^2$\n",
    "\n",
    "<img src=\"../assets/lda_blei.png\" alt=\"The intuitions behind LDA (Blei 2012)\" width=\"1000\" height=\"500\"/>\n",
    "\n",
    "$^1$ Blei, D. M., A. Y. Ng, and M. I. Jordan (2003). Latent Dirichlet allocation. _Journal of Machine\n",
    "Learning Research_ 3, 993–1022. <br/>\n",
    "$^2$ Blei, D. M. (2003) Probabilistic Topic Models. _Communications of the ACM_ 55, 77–84."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## How to join in with coding <a id='joinin'></a>\n",
    "\n",
    "* **Edit** any cell and try changing the code, or delete it and write your own.\n",
    "\n",
    "* Before running a cell, think through the code and try to **guess** what output you'll get.\n",
    "\n",
    "* If you encounter an **error**, take a breath; this is normal. Errors happen all the time, and by reading the error message you will often learn something new and useful.\n",
    "\n",
    "* Remember: we are in the cloud, so you cannot break the notebook or your computer: **don't be afraid to experiment!**.\n",
    "\n",
    "**Let's get coding!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pandas dataframe: Children's literature <a id='data'></a>\n",
    "\n",
    "As our toy dataset, let's use a database of children's literature from the 19th century. The data were compiled by students in [this course](http://english197s2015.pbworks.com/w/page/93127947/FrontPage) and have been minimally cleaned for our use. The data and other corpora can be [found here](http://dhresourcesforprojectbuilding.pbworks.com/w/page/69244469/Data%20Collections%20and%20Datasets#demo-corpora), feel free to explore that data and play with it using text analysis!\n",
    "\n",
    "First, we read our corpus into a Pandas dataframe. The file is stored as a compressed .csv file in the `day-1/` folder, so we tell Pandas the compression type (`.bz2`) so it can unpack and read it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author gender</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Dog with a Bad Name</td>\n",
       "      <td>Male</td>\n",
       "      <td>1886</td>\n",
       "      <td>A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Final Reckoning</td>\n",
       "      <td>Male</td>\n",
       "      <td>1887</td>\n",
       "      <td>A Final Reckoning: A Tale of Bush Life in Aust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A House Party, Don Gesualdo, and A Rainy June</td>\n",
       "      <td>Female</td>\n",
       "      <td>1887</td>\n",
       "      <td>A HOUSE-PARTY  Don Gesualdo  and  A Rainy June...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Houseful of Girls</td>\n",
       "      <td>Female</td>\n",
       "      <td>1889</td>\n",
       "      <td>A HOUSEFUL OF GIRLS. BY SARAH TYTLER,  AUTHOR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Little Country Girl</td>\n",
       "      <td>Female</td>\n",
       "      <td>1885</td>\n",
       "      <td>LITTLE COUNTRY GIRL.  BY  SUSAN COOLIDGE,     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Up the River</td>\n",
       "      <td>Male</td>\n",
       "      <td>1881</td>\n",
       "      <td>UP THE RIVER  OR  YACHTING ON THE MISSISSIPPI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>What Katy Did Next</td>\n",
       "      <td>Female</td>\n",
       "      <td>1886</td>\n",
       "      <td>WHAT KATY DID NEXT  BY  SUSAN COOLIDGE    This...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Winning His Spurs</td>\n",
       "      <td>Male</td>\n",
       "      <td>1882</td>\n",
       "      <td>WINNING HIS SPURS            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>With Clive in India</td>\n",
       "      <td>Male</td>\n",
       "      <td>1884</td>\n",
       "      <td>WITH CLIVE IN INDIA:  Or, The Beginnings of an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>With Wolfe in Canada</td>\n",
       "      <td>Male</td>\n",
       "      <td>1887</td>\n",
       "      <td>\\n WITH WOLFE IN CANADA  Or The Winning of a C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title author gender  year  \\\n",
       "0                            A Dog with a Bad Name          Male  1886   \n",
       "1                                A Final Reckoning          Male  1887   \n",
       "2    A House Party, Don Gesualdo, and A Rainy June        Female  1887   \n",
       "3                              A Houseful of Girls        Female  1889   \n",
       "4                            A Little Country Girl        Female  1885   \n",
       "..                                             ...           ...   ...   \n",
       "127                                   Up the River          Male  1881   \n",
       "128                             What Katy Did Next        Female  1886   \n",
       "129                              Winning His Spurs          Male  1882   \n",
       "130                            With Clive in India          Male  1884   \n",
       "131                           With Wolfe in Canada          Male  1887   \n",
       "\n",
       "                                                  text  \n",
       "0    A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...  \n",
       "1    A Final Reckoning: A Tale of Bush Life in Aust...  \n",
       "2    A HOUSE-PARTY  Don Gesualdo  and  A Rainy June...  \n",
       "3    A HOUSEFUL OF GIRLS. BY SARAH TYTLER,  AUTHOR ...  \n",
       "4    LITTLE COUNTRY GIRL.  BY  SUSAN COOLIDGE,     ...  \n",
       "..                                                 ...  \n",
       "127  UP THE RIVER  OR  YACHTING ON THE MISSISSIPPI ...  \n",
       "128  WHAT KATY DID NEXT  BY  SUSAN COOLIDGE    This...  \n",
       "129                   WINNING HIS SPURS            ...  \n",
       "130  WITH CLIVE IN INDIA:  Or, The Beginnings of an...  \n",
       "131  \\n WITH WOLFE IN CANADA  Or The Winning of a C...  \n",
       "\n",
       "[127 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_lit = pd.read_csv(\"../assets/childrens_lit.csv.bz2\", \n",
    "                     sep='\\t', \n",
    "                     index_col=0, \n",
    "                     encoding = 'utf-8', \n",
    "                     compression='bz2')\n",
    "\n",
    "#drop rows where the text is missing.\n",
    "df_lit = df_lit.dropna(subset=['text'])\n",
    "\n",
    "#view the dataframe\n",
    "df_lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data <a id='explore'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some descriptive statistics about this dataset, to get a feel for what's in it and check for any visible errors. This will help with interpreting the data and knowing what kinds of questions to ask. \n",
    "\n",
    "The first thing most people do is to `describe` their data. (This is the `summary` command in R, or the `sum` command in Stata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1885.110236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.752281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1880.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1883.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1886.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1887.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1889.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              year\n",
       "count   127.000000\n",
       "mean   1885.110236\n",
       "std       2.752281\n",
       "min    1880.000000\n",
       "25%    1883.000000\n",
       "50%    1886.000000\n",
       "75%    1887.000000\n",
       "max    1889.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There's only one numeric column in our data ('year'), so we only get one column for output.\n",
    "df_lit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the distribution of the 'year' column\n",
    "import matplotlib\n",
    "df_lit['year'].hist(\n",
    "    bins = 10, \n",
    "    grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the breakdown of author genders in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Male      99\n",
       "Female    28\n",
       "Name: author gender, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can count this using the value_counts() function\n",
    "df_lit['author gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author gender</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>A Dog with a Bad Name</td>\n",
       "      <td>Male</td>\n",
       "      <td>A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title author gender  \\\n",
       "count                     127           127   \n",
       "unique                    127             2   \n",
       "top     A Dog with a Bad Name          Male   \n",
       "freq                        1            99   \n",
       "\n",
       "                                                     text  \n",
       "count                                                 127  \n",
       "unique                                                127  \n",
       "top     A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This only gets us numerical summaries. To get summaries of some of the other columns, we can explicitly ask for it.\n",
    "df_lit.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of titles do these books have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A Dog with a Bad Name                            1\n",
       "The Cornet of Horse                              1\n",
       "The Lion of Saint Mark                           1\n",
       "The Life of a Ship                               1\n",
       "The Land of Mystery                              1\n",
       "The Island Queen                                 1\n",
       "The Happy Prince and Other Stories               1\n",
       "The Gold of Fairnilee                            1\n",
       "The Giant of the North Pokings Round the Pole    1\n",
       "The Fugitives the Tyrant Queen of Madagascar     1\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lit['title'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average year for each author gender? To answer this question, we use the powerful `groupby` function in Pandas. (Similar to `collapse` on Stata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author gender\n",
       "Female    1885.250000\n",
       "Male      1885.070707\n",
       "Name: year, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_grouped_by_gender = df_lit.groupby(\"author gender\")\n",
    "books_grouped_by_gender['year'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing <a id='preprocess'></a>\n",
    "\n",
    ">**The key to understanding Natural Language Processing (NLP) is that the computer can only do computations on _numbers_. We have to present our corpus for analysis in a numerical form&mdash;typically _vectors_&mdash;and make human sense of everything at the end.**\n",
    "\n",
    "A \"bag of words\" corpus is the _vocabulary_ of tokens (words) in the corpus together with some _measure_ of how often they occur. The measurement may be:\n",
    "* binary (presence or absence)\n",
    "* count (how many times the token occurs)\n",
    "* frequency (count divided by the total number of tokens).\n",
    "\n",
    "This is called a \"bag of words\" because the order and location of words is discarded. For example, it doesn't matter if the words 'red' and 'nose' are adjacent ('red nose'), or at the beginning or end of a sentence; this approach just treats the words individually. It's like a 'bag' of Scrabble™ tiles, where each tile is a word, all rattling around together in no particular order. It looks like this:\n",
    "\n",
    "<img src=\"../assets/bag_of_words.png\" alt=\"Bag of words\" width=\"800\" height=\"300\"/>\n",
    "\n",
    "To represent a corpus as a bag of words for topic modeling applications and the like, a common preprocessing step is to create a Document-Term Matrix (DTM). A DTM is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a DTM, rows correspond to documents in the collection and columns correspond to terms. In other words, each document becomes a _vector_ of word frequencies with a value for the count of each word in that document. This is also called Term Frequency (TF) weighting, representing each text by its word counts&mdash;probably the most straightforward method of preparing text data.\n",
    "\n",
    "To turn our texts into numbers in the form of a DTM, let's use the `CountVectorizer` function from `scikit-learn` ([more info here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)). This is a standard method of using DTM in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize our text using CountVectorizer\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.80, min_df=50,\n",
    "                                max_features=None,\n",
    "                                stop_words='english')\n",
    "\n",
    "dtm = tf_vectorizer.fit_transform(df_lit.text)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the DTM we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 547)\t11\n",
      "  (0, 2865)\t14\n",
      "  (0, 2426)\t10\n",
      "  (0, 1903)\t2\n",
      "  (0, 2434)\t7\n",
      "  (0, 2964)\t3\n",
      "  (0, 1742)\t10\n",
      "  (0, 852)\t4\n",
      "  (0, 460)\t2\n",
      "  (0, 931)\t1\n",
      "  (0, 2875)\t13\n",
      "  (0, 1899)\t4\n",
      "  (0, 1980)\t3\n",
      "  (0, 2329)\t8\n",
      "  (0, 2214)\t5\n",
      "  (0, 2181)\t15\n",
      "  (0, 1679)\t2\n",
      "  (0, 504)\t2\n",
      "  (0, 2712)\t4\n",
      "  (0, 2154)\t2\n",
      "  (0, 2635)\t6\n",
      "  (0, 169)\t2\n",
      "  (0, 1969)\t9\n",
      "  (0, 628)\t1\n",
      "  (0, 473)\t6\n",
      "  :\t:\n",
      "  (126, 227)\t2\n",
      "  (126, 2062)\t1\n",
      "  (126, 2649)\t4\n",
      "  (126, 1837)\t6\n",
      "  (126, 3079)\t14\n",
      "  (126, 1442)\t6\n",
      "  (126, 0)\t4\n",
      "  (126, 2391)\t1\n",
      "  (126, 410)\t2\n",
      "  (126, 1419)\t1\n",
      "  (126, 532)\t6\n",
      "  (126, 878)\t1\n",
      "  (126, 2978)\t9\n",
      "  (126, 364)\t4\n",
      "  (126, 810)\t1\n",
      "  (126, 2017)\t1\n",
      "  (126, 2165)\t1\n",
      "  (126, 157)\t1\n",
      "  (126, 677)\t2\n",
      "  (126, 1157)\t1\n",
      "  (126, 2828)\t5\n",
      "  (126, 244)\t4\n",
      "  (126, 555)\t14\n",
      "  (126, 2278)\t1\n",
      "  (126, 1230)\t3\n"
     ]
    }
   ],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is called Compressed Sparse Format. It saves a lot of memory to store the DTM in this format, but it is difficult to look at for a human. Compressed Sparse Format stores a matrix that has lots of zeroes, and saves space by omitting the zeroes. Let's look at a sample to see what it looks like WITH the zeroes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 7, 0, 0, 1, 0],\n",
       "       [0, 2, 4, 1, 1, 0, 2, 1, 2, 2],\n",
       "       [0, 1, 0, 0, 0, 2, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 3, 3, 6, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 2, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 5, 0, 4, 2, 0],\n",
       "       [1, 0, 0, 0, 0, 2, 1, 2, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what each number (row) indicates in Compressed Sparse Format? We can access the words themselves through the `CountVectorizer` function `get_feature_names`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000' 'amuse' 'attract' 'bidding' 'bush' 'church' 'conscience' 'crying'\n",
      " 'despite' 'dried' 'eve' 'female' 'freely' 'grim' 'hitting' 'indulge'\n",
      " 'kissing' 'lovely' 'moonlight' 'occasionally' 'perform' 'preparations'\n",
      " 'puzzle' 'remembering' 'roll' 'separate' 'slipping' 'station' 'summons'\n",
      " 'threatening' 'uneasiness' 'wasn']\n"
     ]
    }
   ],
   "source": [
    "print(tf_vectorizer.get_feature_names_out()[::100]) # get every 100th word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the techniques in this lesson, we will first convert this matrix back to a Pandas dataframe, a format that's more intuitive to work with. For larger datasets, you will have to use the Compressed Sparse Format. \n",
    "\n",
    "*Note*: In practice it is rare to transform a DTM into a Pandas dataframe, because of memory issues. If using this notebook as a reference, don't just copy and paste--think about what you're doing to avoid crashing your system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>50</th>\n",
       "      <th>_you_</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>abode</th>\n",
       "      <th>abreast</th>\n",
       "      <th>...</th>\n",
       "      <th>yelling</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yells</th>\n",
       "      <th>yer</th>\n",
       "      <th>yield</th>\n",
       "      <th>yielded</th>\n",
       "      <th>yonder</th>\n",
       "      <th>york</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  20  30  50  _you_  abandoned  ability  abode  abreast  ...  \\\n",
       "0    0   1   0   1   0      7          0        0      1        0  ...   \n",
       "1    0   2   4   1   1      0          2        1      2        2  ...   \n",
       "2    0   1   0   0   0      2          0        1      1        0  ...   \n",
       "3    0   0   1   0   0      3          3        6      0        0  ...   \n",
       "4    0   0   0   0   0      0          0        0      0        1  ...   \n",
       "\n",
       "   yelling  yellow  yells  yer  yield  yielded  yonder  york  youngest  youth  \n",
       "0        1       1      2    0      0        5       1    23         1     14  \n",
       "1        1       0      2    2      0        0       0     0         0      2  \n",
       "2        0       6      0    0      0        2       3     0         1     13  \n",
       "3        1       2      0    0      1        4       0     0         7     13  \n",
       "4        0      16      0    0      1        0       1     4         3      1  \n",
       "\n",
       "[5 rows x 3197 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tf_vectorizer.get_feature_names_out() # save terms in vectorizer for later use \n",
    "\n",
    "long_dtm = pd.DataFrame(dtm.toarray(), \n",
    "                        columns=vocab, \n",
    "                        index=df_lit.index)\n",
    "long_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can we do with a DTM?**\n",
    "\n",
    "Because DTMs have a matrix format, we can perform any matrix algebra or vector manipulation on it, which enables some pretty exciting things (think vector space and Euclidean geometry). But, what do we lose when we represent text in this format? (Hint: Think \"bag of words\".)\n",
    "\n",
    "For a toy example, we can use our long-format DTM to quickly identify the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doctor     5032\n",
       "dick       4857\n",
       "king       4490\n",
       "jack       3787\n",
       "uncle      3680\n",
       "tom        3199\n",
       "ship       2762\n",
       "project    2738\n",
       "army       2735\n",
       "french     2677\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most frequent words for male and female authors?\n",
    "\n",
    "#### TO DO: Add solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "* Print out the most infrequent words rather than the most frequent words. You can look at the [Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/api.html#api-dataframe-stats) for more information.\n",
    "* Print the average number of times each word is used in a book.\n",
    "* Print this out sorted from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "long_dtm.sum().sort_values().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_dtm.mean().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: Add solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a topic model using LDA <a id='train'></a>\n",
    "\n",
    "The first thing we need to do is 'train a model'. These words and concepts come from the (fashionable!) sphere of **machine learning**. A 'model' is a simplified representation of something in the real world that a computer can handle. In topic modeling, the model is a representation of the topics of the corpus.\n",
    "\n",
    "We can say the computer acquires 'experience', or a simplified 'understanding' of what the corpus is about by creating a model of it. We have to train the computer in how to make its model by feeding it training data&mdash;in the case of topic modeling, this data is the text we are interested in.\n",
    "\n",
    "Let's train the model using the `scikit-learn` function `LatentDirichletAllocation`. See [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for more information about this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "n_samples = 2000\n",
    "n_topics = 4\n",
    "n_top_words = 50\n",
    "\n",
    "##This is a function to print out the top words for each topic in a pretty way.\n",
    "#Don't worry too much about understanding every line of this code.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA model with tf features, n_samples=2000 and n_topics=4...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA model with tf features, \"\n",
    "      \"n_samples=%d and n_topics=%d...\"\n",
    "      % (n_samples, n_topics))\n",
    "\n",
    "#define the lda function, with desired options\n",
    "#Check the documentation, linked above, to look through the options\n",
    "lda = LDA(n_components=n_topics, \n",
    "          max_iter=20,\n",
    "          learning_method='online',\n",
    "          learning_offset=80.,\n",
    "          total_samples=n_samples,\n",
    "          random_state=0)\n",
    "#fit the model\n",
    "lda.fit(dtm)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "\n",
      "Topic #0:\n",
      "project doctor girls sister papa mamma london baby sweet tom street works dr remarked aunt tea youth foundation presently study em ain cousin office darling loved ladies wasn everybody public flower observed nurse ma shop snow ye queen class reader ice stairs flowers lovely agreement sisters doesn carriage bell garden\n",
      "\n",
      "Topic #1:\n",
      "dick uncle doctor er jack ain tom den yer fish em rock wolf gun rope lads ha birds rocks beneath ay stream shock tail moments mate excitedly garden eh sand fishing thrust ye nay gazing softly mountain ship tremendous hook bird leg ashore growled penny shore stones farther jump knife\n",
      "\n",
      "Topic #2:\n",
      "king army french troops camp attack officers prince ship john soldiers city village officer guns rode shore regiment march tom fort wounded british boats sword advanced castle jack james indian band marched native prisoners ships arrows france numbers lads forest frank ride presently vessel fought mounted fleet column stream rear\n",
      "\n",
      "Topic #3:\n",
      "jack ye frank deck project uncle ship george shore doctor vessel ain mate lake cabin em ma st john works boats island officer girls passengers sail street church thou south office ha fish ay yer steam soldier ladies regard fleet camp public foundation rope lads illustration bottle pounds stairs crew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the top words per topic, using the function defined above.\n",
    "#Unlike R, which has a built-in function to print top words, we have to write our own for scikit-learn\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, vocab, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a minute to inspect these top words. _What name would you give each of these topics?_ (This is a common task in topic modeling.)\n",
    "\n",
    "> **❗Important Note: Topic models are not perfectly reproducible, so you may see different topics, words, and/or probabilities  from what others see. This means my examples may not match up exactly with what you see when you run the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Copy and paste the above code and fit a new model, `lda_new`, by changing some of the parameters. How does this change the output?\n",
    "\n",
    "Suggestions:\n",
    "1. Change the number of topics. \n",
    "2. Do not remove stop words. \n",
    "3. Change other options, either in the vectorize stage or the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document by Topic Distribution <a id='topics'></a>\n",
    "\n",
    "One thing we may want to do with the output is find the most representative texts for each topic. A simple way to do this (but not memory efficient) is to merge the topic distribution back into the Pandas dataframe.\n",
    "\n",
    "First get the topic distribution array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.27993669e-01, 1.43700382e-02, 5.76027691e-02, 3.35234006e-05],\n",
       "       [2.73383488e-02, 7.01313231e-02, 4.91328141e-01, 4.11202187e-01],\n",
       "       [9.72301526e-01, 3.66126075e-05, 2.25981325e-02, 5.06372922e-03],\n",
       "       [9.99907351e-01, 3.06661128e-05, 3.10550012e-05, 3.09277922e-05],\n",
       "       [9.69292227e-01, 2.88828433e-02, 6.05477628e-05, 1.76438162e-03],\n",
       "       [9.02818435e-01, 9.70583246e-02, 6.16743414e-05, 6.15656757e-05],\n",
       "       [4.10396600e-01, 5.86307314e-01, 1.35979768e-04, 3.16010688e-03],\n",
       "       [9.99890902e-01, 3.60980543e-05, 3.67128035e-05, 3.62872482e-05],\n",
       "       [2.46371338e-01, 2.56589766e-01, 4.50088248e-01, 4.69506480e-02],\n",
       "       [7.08481800e-05, 3.81086663e-01, 6.18771825e-01, 7.06641686e-05],\n",
       "       [6.19417246e-02, 5.60865999e-02, 8.81890242e-01, 8.14334923e-05],\n",
       "       [5.50253267e-05, 5.59319821e-05, 5.52375701e-05, 9.99833805e-01],\n",
       "       [1.54941895e-01, 8.53067338e-05, 1.95440934e-01, 6.49531864e-01],\n",
       "       [8.16413872e-01, 6.08089939e-02, 1.22708631e-01, 6.85031993e-05],\n",
       "       [6.29009722e-01, 4.21386866e-02, 2.54541577e-01, 7.43100148e-02],\n",
       "       [5.20390704e-01, 2.69891665e-05, 3.12178238e-01, 1.67404069e-01],\n",
       "       [2.59339206e-05, 2.54612217e-05, 9.99922584e-01, 2.60211864e-05],\n",
       "       [6.22435512e-03, 4.68816251e-05, 2.25767472e-02, 9.71152016e-01],\n",
       "       [3.87465852e-04, 9.99551674e-01, 3.01743230e-05, 3.06855502e-05],\n",
       "       [2.68953989e-05, 9.99919007e-01, 2.72168308e-05, 2.68810707e-05],\n",
       "       [2.69833700e-05, 2.80524034e-02, 7.49472949e-01, 2.22447664e-01],\n",
       "       [2.77889542e-05, 7.34069452e-02, 1.97857460e-01, 7.28707806e-01],\n",
       "       [9.97249650e-01, 6.12290209e-05, 1.11113084e-03, 1.57799044e-03],\n",
       "       [2.11005150e-03, 7.24263955e-01, 2.73601496e-01, 2.44968641e-05],\n",
       "       [2.23764047e-05, 9.99933009e-01, 2.22828683e-05, 2.23316258e-05],\n",
       "       [4.93753142e-05, 4.91020308e-05, 4.98451013e-05, 9.99851678e-01],\n",
       "       [9.92489652e-01, 1.86473736e-03, 1.54734276e-03, 4.09826791e-03],\n",
       "       [9.99882923e-01, 3.88701433e-05, 3.89527356e-05, 3.92542903e-05],\n",
       "       [9.99873970e-01, 4.19279277e-05, 4.18597810e-05, 4.22421685e-05],\n",
       "       [6.95185897e-04, 7.34312964e-03, 4.01273553e-03, 9.87948949e-01],\n",
       "       [8.75839693e-01, 1.24052565e-01, 5.41207288e-05, 5.36206171e-05],\n",
       "       [6.36222545e-01, 3.63720142e-01, 2.86381710e-05, 2.86742684e-05],\n",
       "       [2.95087877e-05, 2.95816598e-05, 9.99911112e-01, 2.97977155e-05],\n",
       "       [2.13256430e-05, 2.10559479e-05, 9.99936397e-01, 2.12209363e-05],\n",
       "       [2.52951451e-05, 2.47863102e-05, 9.99924710e-01, 2.52088689e-05],\n",
       "       [5.50178330e-01, 3.81124210e-05, 2.19143102e-01, 2.30640456e-01],\n",
       "       [9.99870242e-01, 4.29589552e-05, 4.32499132e-05, 4.35495588e-05],\n",
       "       [7.68274655e-01, 2.31587553e-01, 6.88908253e-05, 6.89012673e-05],\n",
       "       [3.02635323e-05, 3.00362899e-05, 2.28550285e-01, 7.71389415e-01],\n",
       "       [3.78348200e-01, 3.76726709e-05, 6.05171022e-01, 1.64431056e-02],\n",
       "       [2.48140562e-05, 1.14676837e-01, 8.85273702e-01, 2.46464918e-05],\n",
       "       [2.46288680e-05, 6.86021611e-02, 8.78430327e-01, 5.29428835e-02],\n",
       "       [6.96768549e-01, 1.61091998e-02, 2.48347032e-01, 3.87752187e-02],\n",
       "       [9.84724988e-01, 1.32514180e-02, 2.82458747e-05, 1.99534815e-03],\n",
       "       [9.48549600e-01, 2.17851043e-02, 4.99388846e-05, 2.96153565e-02],\n",
       "       [9.99432957e-01, 1.86591148e-04, 1.88885121e-04, 1.91566514e-04],\n",
       "       [6.44725935e-03, 1.15032272e-01, 1.39974344e-01, 7.38546124e-01],\n",
       "       [8.92880314e-01, 1.06990181e-01, 6.49862726e-05, 6.45187573e-05],\n",
       "       [2.79592490e-05, 9.99916066e-01, 2.79014504e-05, 2.80733658e-05],\n",
       "       [1.89458962e-05, 7.47291749e-01, 2.52670376e-01, 1.89295372e-05],\n",
       "       [2.47825533e-05, 9.68102678e-01, 2.02279090e-02, 1.16446302e-02],\n",
       "       [9.40573222e-01, 4.88573281e-02, 6.35625559e-05, 1.05058869e-02],\n",
       "       [6.43259582e-01, 1.48922683e-02, 2.49253922e-05, 3.41823224e-01],\n",
       "       [2.62163326e-05, 9.99921897e-01, 2.57379596e-05, 2.61491059e-05],\n",
       "       [2.75622380e-05, 2.72366214e-05, 9.99917589e-01, 2.76122315e-05],\n",
       "       [2.84026252e-05, 9.93662591e-01, 2.80940235e-05, 6.28091229e-03],\n",
       "       [6.04745270e-05, 5.95408446e-05, 5.84978728e-05, 9.99821487e-01],\n",
       "       [5.93806852e-02, 3.46949883e-05, 3.43401887e-05, 9.40550280e-01],\n",
       "       [9.99890704e-01, 3.67775412e-05, 3.61683120e-05, 3.63499455e-05],\n",
       "       [9.15079355e-01, 2.58967910e-05, 1.62305819e-02, 6.86641667e-02],\n",
       "       [3.57347337e-01, 1.41755892e-04, 6.42369659e-01, 1.41247656e-04],\n",
       "       [1.34751191e-01, 8.65194234e-01, 2.71253760e-05, 2.74492035e-05],\n",
       "       [8.18693885e-01, 4.46272497e-03, 1.76816512e-01, 2.68784146e-05],\n",
       "       [9.99607042e-01, 3.79842903e-05, 3.79466263e-05, 3.17027013e-04],\n",
       "       [9.78565311e-01, 2.09382720e-02, 2.48876622e-04, 2.47540480e-04],\n",
       "       [3.33633353e-01, 7.07487297e-02, 5.92078991e-01, 3.53892572e-03],\n",
       "       [1.56793327e-01, 6.90749086e-01, 2.54887811e-04, 1.52202700e-01],\n",
       "       [2.34909275e-05, 2.30992456e-05, 9.99930092e-01, 2.33182901e-05],\n",
       "       [1.24520919e-02, 6.07831844e-02, 7.03609668e-02, 8.56403757e-01],\n",
       "       [5.36809469e-05, 5.27821583e-05, 1.81841629e-01, 8.18051907e-01],\n",
       "       [1.62000362e-01, 2.70566965e-01, 5.66232408e-01, 1.20026461e-03],\n",
       "       [4.75872869e-01, 5.23997278e-01, 6.44584727e-05, 6.53940160e-05],\n",
       "       [5.11255638e-01, 6.63880256e-02, 1.29577789e-01, 2.92778547e-01],\n",
       "       [3.25555282e-01, 5.29041078e-05, 6.64877341e-01, 9.51447358e-03],\n",
       "       [6.74521206e-01, 3.94834898e-02, 2.82683303e-01, 3.31200138e-03],\n",
       "       [6.59971779e-01, 3.26800005e-01, 1.92630251e-04, 1.30355853e-02],\n",
       "       [4.47955889e-01, 2.07716120e-05, 5.52002729e-01, 2.06111200e-05],\n",
       "       [2.74230398e-05, 2.73114627e-05, 9.48652379e-01, 5.12928869e-02],\n",
       "       [3.24002453e-02, 2.43931794e-05, 9.67550760e-01, 2.46015027e-05],\n",
       "       [9.94149674e-01, 3.97325275e-05, 5.77114770e-03, 3.94457639e-05],\n",
       "       [5.07795452e-03, 2.60262231e-05, 9.94869727e-01, 2.62918616e-05],\n",
       "       [5.08686499e-01, 8.99641260e-02, 3.05305812e-01, 9.60435633e-02],\n",
       "       [3.63808909e-01, 3.12648813e-05, 5.60073674e-01, 7.60861521e-02],\n",
       "       [2.87962449e-05, 2.86650849e-05, 9.99913614e-01, 2.89245921e-05],\n",
       "       [6.44128252e-01, 1.62680401e-01, 1.00259212e-01, 9.29321352e-02],\n",
       "       [9.99914405e-01, 2.87026734e-05, 2.83997990e-05, 2.84923735e-05],\n",
       "       [5.96775142e-01, 2.53240034e-05, 4.03174376e-01, 2.51587580e-05],\n",
       "       [6.06937332e-01, 1.54511233e-01, 2.32583613e-01, 5.96782155e-03],\n",
       "       [2.52271767e-01, 2.30484781e-01, 2.10400343e-01, 3.06843109e-01],\n",
       "       [8.30419324e-01, 6.65000717e-02, 1.02893114e-01, 1.87490356e-04],\n",
       "       [6.57108783e-01, 1.21895883e-02, 1.59472791e-01, 1.71228837e-01],\n",
       "       [2.35593547e-01, 5.21793079e-02, 7.12186383e-01, 4.07624485e-05],\n",
       "       [5.07083732e-03, 2.12332011e-04, 2.11909485e-04, 9.94504921e-01],\n",
       "       [2.60719523e-05, 2.57156990e-05, 9.91388118e-01, 8.56009414e-03],\n",
       "       [2.33063082e-05, 2.31460419e-05, 9.99930270e-01, 2.32780864e-05],\n",
       "       [9.13593348e-01, 7.73206543e-02, 8.98878874e-03, 9.72093796e-05],\n",
       "       [3.50812124e-01, 5.86339435e-02, 6.90240031e-05, 5.90484908e-01],\n",
       "       [9.99903337e-01, 3.22304386e-05, 3.22208712e-05, 3.22113930e-05],\n",
       "       [8.38573910e-01, 1.60767632e-02, 1.22366608e-01, 2.29827186e-02],\n",
       "       [3.00244254e-02, 8.74038640e-01, 9.58471271e-02, 8.98078334e-05],\n",
       "       [9.99897778e-01, 3.38302806e-05, 3.40937077e-05, 3.42982466e-05],\n",
       "       [6.18421796e-01, 2.82558672e-02, 3.53277967e-01, 4.43698230e-05],\n",
       "       [3.66632194e-01, 1.67964686e-01, 4.65343321e-01, 5.97993334e-05],\n",
       "       [5.74429841e-01, 1.14779581e-01, 3.07629681e-01, 3.16089710e-03],\n",
       "       [2.98722530e-05, 7.80221211e-01, 2.19719235e-01, 2.96820595e-05],\n",
       "       [6.85523511e-01, 1.99822730e-01, 2.49746058e-04, 1.14404013e-01],\n",
       "       [7.07904733e-01, 3.67137105e-02, 2.07369327e-01, 4.80122293e-02],\n",
       "       [9.93605752e-01, 9.26695521e-05, 8.95035000e-05, 6.21207458e-03],\n",
       "       [9.99869696e-01, 4.34422188e-05, 4.32532342e-05, 4.36086232e-05],\n",
       "       [9.89787950e-01, 2.77545432e-05, 1.01567162e-02, 2.75792374e-05],\n",
       "       [4.69569319e-03, 3.84249969e-02, 9.56853604e-01, 2.57059858e-05],\n",
       "       [2.14651385e-05, 2.12966427e-05, 9.99935882e-01, 2.13565648e-05],\n",
       "       [3.21889729e-05, 1.32252402e-01, 8.67683173e-01, 3.22364414e-05],\n",
       "       [5.23945561e-01, 2.35792644e-02, 3.03667792e-05, 4.52444808e-01],\n",
       "       [3.47794382e-05, 9.99895593e-01, 3.46976144e-05, 3.49300271e-05],\n",
       "       [8.24753592e-05, 9.99750816e-01, 8.35146626e-05, 8.31942505e-05],\n",
       "       [4.28114427e-01, 1.87053683e-03, 1.33569490e-01, 4.36445546e-01],\n",
       "       [4.54667623e-03, 3.63829509e-01, 5.93609748e-02, 5.72262840e-01],\n",
       "       [5.37666487e-01, 1.82696997e-01, 2.16645746e-01, 6.29907702e-02],\n",
       "       [6.29197555e-02, 3.24250187e-01, 6.06466466e-01, 6.36359138e-03],\n",
       "       [3.39127111e-03, 9.96494489e-01, 5.63670954e-05, 5.78726307e-05],\n",
       "       [2.24936092e-05, 2.26005723e-05, 9.99932315e-01, 2.25905995e-05],\n",
       "       [4.71212890e-05, 4.68654488e-05, 4.78505333e-05, 9.99858163e-01],\n",
       "       [9.60204564e-01, 5.35638494e-05, 5.19090084e-03, 3.45509713e-02],\n",
       "       [2.58383298e-05, 2.54067521e-05, 9.99923113e-01, 2.56418862e-05],\n",
       "       [2.00457030e-05, 2.02350450e-05, 9.99939520e-01, 2.01988375e-05],\n",
       "       [2.23771449e-05, 2.23741816e-05, 9.67389742e-01, 3.25655066e-02]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dist = lda.transform(dtm)\n",
    "topic_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge back in with the original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>title</th>\n",
       "      <th>author gender</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.927994</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>0.057603</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>A Dog with a Bad Name</td>\n",
       "      <td>Male</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027338</td>\n",
       "      <td>0.070131</td>\n",
       "      <td>0.491328</td>\n",
       "      <td>0.411202</td>\n",
       "      <td>A Final Reckoning</td>\n",
       "      <td>Male</td>\n",
       "      <td>1887.0</td>\n",
       "      <td>A Final Reckoning: A Tale of Bush Life in Aust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972302</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>A House Party, Don Gesualdo, and A Rainy June</td>\n",
       "      <td>Female</td>\n",
       "      <td>1887.0</td>\n",
       "      <td>A HOUSE-PARTY  Don Gesualdo  and  A Rainy June...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999907</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>A Houseful of Girls</td>\n",
       "      <td>Female</td>\n",
       "      <td>1889.0</td>\n",
       "      <td>A HOUSEFUL OF GIRLS. BY SARAH TYTLER,  AUTHOR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.969292</td>\n",
       "      <td>0.028883</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>A Little Country Girl</td>\n",
       "      <td>Female</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>LITTLE COUNTRY GIRL.  BY  SUSAN COOLIDGE,     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.999858</td>\n",
       "      <td>Treasure Island</td>\n",
       "      <td>Male</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>TREASURE ISLAND  by Robert Louis Stevenson    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.960205</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.034551</td>\n",
       "      <td>Twice Bought</td>\n",
       "      <td>Male</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>The Project Gutenberg EBook of Twice Bought, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>Two Arrows</td>\n",
       "      <td>Male</td>\n",
       "      <td>1886.0</td>\n",
       "      <td>TWO ARROWS      HARPER'S YOUNG PEOPLE'S SERIES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>Uncle Remus: His Songs and Sayings</td>\n",
       "      <td>Male</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>Uncle Remus: His Songs and His Sayings  By Joe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.967390</td>\n",
       "      <td>0.032566</td>\n",
       "      <td>Under Drake's Flag</td>\n",
       "      <td>Male</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>Under Drake's Flag:  A Tale of the Spanish Mai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3  \\\n",
       "0    0.927994  0.014370  0.057603  0.000034   \n",
       "1    0.027338  0.070131  0.491328  0.411202   \n",
       "2    0.972302  0.000037  0.022598  0.005064   \n",
       "3    0.999907  0.000031  0.000031  0.000031   \n",
       "4    0.969292  0.028883  0.000061  0.001764   \n",
       "..        ...       ...       ...       ...   \n",
       "122  0.000047  0.000047  0.000048  0.999858   \n",
       "123  0.960205  0.000054  0.005191  0.034551   \n",
       "124  0.000026  0.000025  0.999923  0.000026   \n",
       "125  0.000020  0.000020  0.999940  0.000020   \n",
       "126  0.000022  0.000022  0.967390  0.032566   \n",
       "\n",
       "                                             title author gender    year  \\\n",
       "0                            A Dog with a Bad Name          Male  1886.0   \n",
       "1                                A Final Reckoning          Male  1887.0   \n",
       "2    A House Party, Don Gesualdo, and A Rainy June        Female  1887.0   \n",
       "3                              A Houseful of Girls        Female  1889.0   \n",
       "4                            A Little Country Girl        Female  1885.0   \n",
       "..                                             ...           ...     ...   \n",
       "122                                Treasure Island          Male  1883.0   \n",
       "123                                   Twice Bought          Male  1885.0   \n",
       "124                                     Two Arrows          Male  1886.0   \n",
       "125             Uncle Remus: His Songs and Sayings          Male  1880.0   \n",
       "126                             Under Drake's Flag          Male  1883.0   \n",
       "\n",
       "                                                  text  \n",
       "0    A DOG WITH A BAD NAME  BY TALBOT BAINES REED  ...  \n",
       "1    A Final Reckoning: A Tale of Bush Life in Aust...  \n",
       "2    A HOUSE-PARTY  Don Gesualdo  and  A Rainy June...  \n",
       "3    A HOUSEFUL OF GIRLS. BY SARAH TYTLER,  AUTHOR ...  \n",
       "4    LITTLE COUNTRY GIRL.  BY  SUSAN COOLIDGE,     ...  \n",
       "..                                                 ...  \n",
       "122  TREASURE ISLAND  by Robert Louis Stevenson    ...  \n",
       "123  The Project Gutenberg EBook of Twice Bought, b...  \n",
       "124  TWO ARROWS      HARPER'S YOUNG PEOPLE'S SERIES...  \n",
       "125  Uncle Remus: His Songs and His Sayings  By Joe...  \n",
       "126  Under Drake's Flag:  A Tale of the Spanish Mai...  \n",
       "\n",
       "[127 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dist_df = pd.DataFrame(topic_dist)\n",
    "df_w_topics = topic_dist_df.join(df_lit)\n",
    "df_w_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sort the dataframe for the topic of interest, and view the top documents for the topics.\n",
    "Below we sort the documents first by Topic 0 (looking at the top words for this topic I think it's about family, health, and domestic activities), and next by Topic 1 (again looking at the top words I think this topic is about children playing outside in nature). These topics may be a family/nature split?\n",
    "\n",
    "Look at the titles for the two different topics. Look at the gender of the author. Hypotheses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  title author gender         0\n",
      "85                                  NaN           NaN  0.999914\n",
      "3                   A Houseful of Girls        Female  0.999907\n",
      "97                   The Life of a Ship          Male  0.999903\n",
      "100   The Little Princess of Tower Hill        Female  0.999898\n",
      "7                      A World of Girls        Female  0.999891\n",
      "..                                  ...           ...       ...\n",
      "24                     Dick o' the Fens          Male  0.000022\n",
      "111                The Thorogood Family          Male  0.000021\n",
      "33                       For the Temple          Male  0.000021\n",
      "125  Uncle Remus: His Songs and Sayings          Male  0.000020\n",
      "49                            Menhardoc          Male  0.000019\n",
      "\n",
      "[127 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_w_topics[['title', 'author gender', 0]].sort_values(by=[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  title author gender         1\n",
      "24                     Dick o' the Fens          Male  0.999933\n",
      "53                      My Friend Smith          Male  0.999922\n",
      "19                          Bunyip Land          Male  0.999919\n",
      "48               Little Lord Fauntleroy        Female  0.999916\n",
      "114             The Willoughby Captains          Male  0.999896\n",
      "..                                  ...           ...       ...\n",
      "126                  Under Drake's Flag          Male  0.000022\n",
      "111                The Thorogood Family          Male  0.000021\n",
      "33                       For the Temple          Male  0.000021\n",
      "76                        The Big Otter          Male  0.000021\n",
      "125  Uncle Remus: His Songs and Sayings          Male  0.000020\n",
      "\n",
      "[127 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_w_topics[['title', 'author gender', 1]].sort_values(by=[1], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "- What is the average topic weight by author gender, for each topic?\n",
    "- Which topic is most represented in texts by women? Most represented in texts by men?\n",
    "- Which topic is least represented in texts by women? Least represented in texts by men?\n",
    "- Graph these results.\n",
    "\n",
    "Hint 1: Consider using the python `range` function and a for-loop to create a list of topic indices and inspect average topic weights. This code block gets that started for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "topic_columns = range(0,4)\n",
    "for num in topic_columns:\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint 2: Use a Pandas `groupby()` to compare the topic loadings of Male and Female authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "author gender\n",
      "Female    0.653199\n",
      "Male      0.323377\n",
      "Name: 0, dtype: float64\n",
      "1\n",
      "author gender\n",
      "Female    0.127721\n",
      "Male      0.189827\n",
      "Name: 1, dtype: float64\n",
      "2\n",
      "author gender\n",
      "Female    0.157666\n",
      "Male      0.326972\n",
      "Name: 2, dtype: float64\n",
      "3\n",
      "author gender\n",
      "Female    0.061414\n",
      "Male      0.159824\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#solution\n",
    "grouped = df_w_topics.groupby(\"author gender\")\n",
    "for num in topic_columns:\n",
    "    print(num)\n",
    "    print(grouped[num].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig1 = plt.figure()\n",
    "chrt = 0\n",
    "for num in topic_columns:\n",
    "    chrt += 1 \n",
    "    ax = fig1.add_subplot(2,3, chrt)\n",
    "    grouped[num].mean().plot(\n",
    "        kind = 'bar', \n",
    "        yerr = grouped[num].std(), \n",
    "        ylim=0, ax=ax, title=num)\n",
    "\n",
    "fig1.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA as dimensionality reduction <a id='dimensionality'></a>\n",
    "\n",
    "Another use for topic modeling is to reduce the dimensionality of text. Instead of representing a document as a vector of token counts over the whole vocabulary (which may be very large), topic modeling can represent that document as a weighted average of _k_ topics over a finite number of (more or less) interpretable topics. The latter approach is usually more tractable for subsequent statistical analysis (linear regression, decision trees, transformers, etc). \n",
    "\n",
    "Now that we obtained a distribution of topic weights for each document, we can represent our corpus with a dense document-weight matrix as opposed to our initial sparse DTM. The weights can then replace tokens as features for any subsequent task (classification, prediction, etc). A simple example may consist in measuring cosine similarity between documents. For instance, which book is closest to the first book in our corpus? Let's use pairwise cosine similarity to find out. \n",
    "\n",
    "NB: cosine similarity measures an angle between two vectors, which provides a measure of distance robust to vectors of different lenghts (total number of tokens)\n",
    "\n",
    "First, let's create a sparse DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tf_vectorizer.fit_transform(df_lit['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's import the `cosine_similarity` function from `sklearn` and print the cosine similarity between the first and second book or the first and third book. Let's use the built-in `time` function to tell us how long this takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between first and second book:  [[0.28012828]]\n",
      "Cosine similarity between first and third book:  [[0.40808192]]\n",
      "\n",
      "Computing the above code took 0.012564659118652344 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print(\"Cosine similarity between first and second book: \", \n",
    "      str(cosine_similarity(dtm[0,:], dtm[1,:])))\n",
    "print(\"Cosine similarity between first and third book: \", \n",
    "      str(cosine_similarity(dtm[0,:], dtm[2,:])))\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print(\"Computing the above code took \" + str(end-start) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we use the topic weights instead of word frequencies? Would this get similar results, and would it be faster? \n",
    "\n",
    "To try this out, it's more straightforward to calculate cosine similarity with the `spatial` function from `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between first and second book:  0.09117452962871253\n",
      "Cosine similarity between first and third book:  0.9991170178858493\n",
      "\n",
      "Computing the above code took 0.0049397945404052734 seconds.\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "dwm = df_w_topics.iloc[:,:4] # limit docs/topics DF to only topic weights\n",
    "\n",
    "start = time.time()\n",
    "print(\"Cosine similarity between first and second book: \", \n",
    "      str(1-spatial.distance.cosine(dwm.iloc[0,:], dwm.iloc[1,:])))\n",
    "print(\"Cosine similarity between first and third book: \", \n",
    "      str(1-spatial.distance.cosine(dwm.iloc[0,:], dwm.iloc[2,:])))\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print(\"Computing the above code took \" + str(end-start) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like using topic weights to compute similarities between a few documents took less than half the time than did using word frequencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Calculate the cosine similarity between the first book and all other books. What is the most similar book to the first one?\n",
    "\n",
    "_Hint 1:_ Use the `df.iterrows()` method to iterate over all rows in the document/topic weights DataFrame we just made. \n",
    "\n",
    "_Hint 2:_ To store the similarities, consider using a Python dictionary&mdashthat is, a list of key-value pairs. `max()` will get you the highest entry in the dictionary, and `dict.values()` will get you the dictionary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarities...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "first_book = dwm.iloc[0,:] # separate out topic weights for first book\n",
    "\n",
    "similarities = {} # initialize dictionary\n",
    "\n",
    "print(\"Computing similarities...\")\n",
    "for book_idx, row in dwm.iloc[1:,].iterrows(): # loop over rows after first row, as we're comparing all books to first one\n",
    "    sim = 1-spatial.distance.cosine(first_book, row)\n",
    "    similarities[book_idx] = sim\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a score of 0.99912,\n",
      "the most similar book to 'A Dog with a Bad Name' is:\n",
      "'A House Party, Don Gesualdo, and A Rainy June'\n"
     ]
    }
   ],
   "source": [
    "# Get similarity and index of most similar book\n",
    "max_sim = max(similarities.values())\n",
    "max_idx = max(similarities, key = similarities.get)\n",
    "\n",
    "# Print the results\n",
    "print(\"With a score of \" + str(round(max_sim,5)) + \",\")\n",
    "print(\"the most similar book to '\" + df_lit[\"title\"].iloc[0] + \"' is:\")\n",
    "print(\"'\" + df_lit[\"title\"].iloc[max_idx] + \"'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
