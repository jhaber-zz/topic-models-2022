{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Topic Modeling\n",
    "## Day 2: Visualization and Evaluation\n",
    "---\n",
    "---\n",
    "\n",
    "## Additional learning goals\n",
    "\n",
    "* Understand Term Frequency–Inverse Document Frequency (TF-IDF) scores and why they are useful\n",
    "* Understand stemming and how to implement it\n",
    "* Get practice with creating word clouds from topic words \n",
    "* Understand different methods to calculate topic prevalence\n",
    "* Learn how to create some simple graphs with topic prevalence\n",
    "* Learn how to visualize topics with pyLDAvis\n",
    "* Learn about a few metrics for topic model evaluation\n",
    "\n",
    "\n",
    "## Outline\n",
    "- [Load the data](#data)\n",
    "- [Vectorize and train](#train)\n",
    "    - [Stemming](#stem)\n",
    "- [Visualize topic words with `wordcloud`](#cloud)\n",
    "- [Words aligned with each topic](#words)\n",
    "- [Topic prevalence over time](#time)\n",
    "- [Visualising topics with pyLDAvis](#viz)\n",
    "- [Evaluating the topic model](#eval)\n",
    "- [Resources and alternatives](#resources)\n",
    "\n",
    "\n",
    "## Key Terms\n",
    "* *coherence*:\n",
    "    * The conditional likelihood of the co-occurrence of words in a topic. The higher the coherence, the more a topic model reflects human interpretation, i.e. expert annotation.\n",
    "* *likelihood*:\n",
    "    * A measure of the probability of the observed data, given the model — i.e. how well a topic model fits the observed data. The higher the likelihood, the better the model for the given data.\n",
    "* *perplexity*:\n",
    "    * A measure of how well a model predicts a sample, i.e. how much it is “perplexed” by a sample from the observed data. The lower the score, the better the model for the given data.\n",
    "* *stemming*:\n",
    "    * A method of text preprocessing that simplifies the language corpus and improves the cleanness of results by removing the ends of words. A common algorithm for this is the PorterStemmeer. Stemming is distinct from lemmatization, which converts words into a base form and is more reliable&mdash;but is also usually slower and more computationally demanding.\n",
    "* *TF-IDF Scores*: \n",
    "    * short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "* *word cloud*:\n",
    "    * a simple visualization of word frequency in a text corpus, where words are scaled by frequency. These often have appealing aesthetic properties like pretty fonts or colors, but they tell us little about the data other than word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "As always, first we load the data. We'll use the same dataset of children's literature described yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_lit = pd.read_csv(\"../assets/childrens_lit.csv.bz2\", sep='\\t', index_col=0, encoding = 'utf-8', compression='bz2')\n",
    "df_lit = df_lit.dropna(subset=['text']) # drop where missing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize and train <a id='train'></a>\n",
    "\n",
    "Yesterday, we used `scikit-learn`'s `CountVectorizer` to build a document-term matrix (DTM) in preparation for topic modeling. This used simple term counts and a bag of words approach to turn texts into numbers&mdash;also known as text vectorization. As a reference, here is the code to load the data, vectorize the texts by term frequencies, and train an LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.80, min_df=50,\n",
    "                                max_features=None,\n",
    "                                stop_words='english')\n",
    "\n",
    "# Create sparse DTM\n",
    "tf_dtm = tf_vectorizer.fit_transform(df_lit.text)\n",
    "\n",
    "tf_vocab = tf_vectorizer.get_feature_names() # Save vocabulary for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "n_samples = 2000\n",
    "n_topics = 4\n",
    "n_top_words = 50\n",
    "\n",
    "print(\"Fitting LDA model with tf features, \"\n",
    "      \"n_samples=%d and n_topics=%d...\"\n",
    "      % (n_samples, n_topics))\n",
    "\n",
    "tf_lda = LDA(n_components=n_topics, \n",
    "          max_iter=20,\n",
    "          learning_method='online',\n",
    "          learning_offset=80.,\n",
    "          total_samples=n_samples,\n",
    "          random_state=0)\n",
    "\n",
    "#fit the model\n",
    "tf_lda.fit(tf_dtm)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    '''Prints the top words for each topic in a pretty way.'''\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "# print top words\n",
    "print(\"\\nTopics in LDA model with TF features and %d topics:\" % n_topics)\n",
    "print_top_words(tf_lda, tf_vocab, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build on yesterday's code by using a more nuanced text vectorization method: `term frequency inverse document frequency (TF-IDF)` scores, which give a word greater weight both when it is more frequent in a text AND when it is rare across the corpus. Words that are frequent, but are also used in every single document, will not be distinguishing. We want to identify words that are unevenly distributed across the corpus to identify distinctive words&mdash;while also filtering out common terms like 'the', 'of', and 'and' without manually removing them during preprocessing.\n",
    "\n",
    "Traditionally, the inverse document frequency is calculated as such:\n",
    "\n",
    "number_of_documents / number_documents_with_term\n",
    "\n",
    "so:\n",
    "\n",
    "tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "You can, and often should, normalize the numerator: \n",
    "\n",
    "tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. This function also uses log frequencies, so the numbers will not correspond exactly to the calculations above. We'll use the [scikit-learn calculation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), but a challenge for you: use Pandas to calculate this manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "Use `sklearn`'s `TfidfVectorizer()` function ([more info here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)) to weight features with TF-IDF. Extract the vectorizer vocabulary for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "Train an LDA model using the TF-IDF-weighted. Then use the `print_top_words()` function defined above to display the top words for each topic. Compare with the output we just saw from the model trained using term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming <a id='stem'></a>\n",
    "\n",
    "As an additional improvement in our text preprocessing, we can stem each word in our input texts BEFORE vectorizing. This allows us to clean up our text data by combining different forms of the same word: for instance, \"running\" and \"runs\" both get converted into \"run\". \n",
    "\n",
    "We'll use the PorterStemmer, one of the most basic and common stemming algorithms. The PorterStemmer uses a number of rules to drop word endings&mdash;but in essence, you can think of it as chopping off word endings. For more info on how the PorterStemmer works, see [here](https://tartarus.org/martin/PorterStemmer/). Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Example stemming application\n",
    "toy_sentence = \"longer words are more long than shorter words\"\n",
    "for word in toy_sentence.split():\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Implement stemming with the PorterStemmer before vectorizing the text. Then construct an LDA model and compare the results to what you saw before.\n",
    "\n",
    "_Hint:_ Create a new DF column called `text_stemmed` with the stemmed version of the book text. Use an `apply()` function with a list comprehension to implement the stemming algorithm to each word in each text. The basic format for this is:\n",
    "\n",
    "```python\n",
    "df[col_new] = df[col].apply(lambda doc: ' '.join([function(word) for word in doc.split()]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize topic words with `wordcloud` <a id='cloud'></a>\n",
    "\n",
    "To get a feel for the most frequent words in each topic, let's create word clouds. Word clouds are simple visualizations of the words in a group of words (like a document, corpus, or topic) with appealing aesthetic properties like pretty fonts or colors. They scale words by frequency, but otherwise they don't tell us much about the text data. Here's an example of a word cloud:\n",
    "\n",
    "<img src=\"../assets/wordcloud.png\" alt=\"Wordcloud\" width=\"700\" height=\"700\"/>\n",
    "\n",
    "To visualize the top words for each topic, let's use the `wordcloud` package (for more info, see [here](https://amueller.github.io/word_cloud/)) together with the topic loadings stored in the `lda` model as `.components_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To foster reproducibility and clean code, let's implement `wordcloud` as a function&mdash;with an informative docstring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def display_wordcloud(model, feature_names, terms_count, save=False):\n",
    "    '''Creates a word cloud with specified # terms for each topic in input topic model. \n",
    "    Credit for example code: Krunal on Medium: https://medium.com/@krunal18/topic-modeling-with-latent-dirichlet-allocation-lda-decomposition-scikit-learn-and-wordcloud-1ff0b8e8a8eb)\n",
    "    \n",
    "    Args:\n",
    "        model (object): topic model from LatentDirichletAllocation (like lda)\n",
    "        feature_names (array): vocabulary from text vectorizer\n",
    "        terms_count (int): number of terms to include for each topic's word cloud\n",
    "        save (binary): whether to display in notebook or save wordclouds to disk\n",
    "    \n",
    "    Returns:\n",
    "        wordcloud visualizations'''\n",
    "\n",
    "    for idx,topic in enumerate(model.components_): # loop over topics\n",
    "        print('Topic# ',idx+1)\n",
    "        \n",
    "        # Get N top words for each topic as a list of lists\n",
    "        topic_terms_sorted = [[feature_names[i], topic[i]] for i in topic.argsort()[:-terms_count - 1:-1]]\n",
    "\n",
    "        topic_words = []\n",
    "\n",
    "        # Print top words above each wordcloud\n",
    "        for i in range(terms_count):\n",
    "            topic_words.append(topic_terms_sorted[i][0])\n",
    "        print(','.join( word for word in topic_words))\n",
    "        print()\n",
    "\n",
    "        dict_word_frequency = {}\n",
    "\n",
    "        for i in range(terms_count):\n",
    "            dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]\n",
    "\n",
    "        # Initialize wordcloud\n",
    "        wcloud = wordcloud.WordCloud(background_color=\"white\",mask=None, max_words=100, \n",
    "                                     max_font_size=100,min_font_size=10,prefer_horizontal=0.9,\n",
    "                                     contour_width=3,contour_color='black', \n",
    "                                     min_word_length=3)\n",
    "\n",
    "        wcloud.generate_from_frequencies(dict_word_frequency)\n",
    "\n",
    "        plt.imshow(wcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Visual done, now save or display\n",
    "        if save:\n",
    "            plt.savefig(\"WordCloud Topic \"+str(idx+1)+\".png\", format=\"png\")\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_size_terms = 50\n",
    "\n",
    "display_wordcloud(tf_lda, tf_vocab, cloud_size_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Create a new LDA model called `tfidf_lda_stemmed` with 10 topics, TF-IDF weighting, and stemmed words. Then create a word cloud for each topic with the top 75 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words aligned with each topic <a id='words'></a>\n",
    "\n",
    "Let's calculate the total number of words aligned with each topic and compare by author gender. This is a way measuring **topic prevalence**, or how frequent each topic is in the corpus.\n",
    "\n",
    "First, we need to merge the topic loadings with the text into one big DataFrame, just like we did yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution and merge with main DataFrame\n",
    "topic_dist = tf_lda.transform(tf_dtm)\n",
    "topic_dist_df = pd.DataFrame(topic_dist)\n",
    "df_w_topics = topic_dist_df.join(df_lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first, create word count column\n",
    "df_w_topics['word_count'] = df_w_topics['text'].apply(lambda x: len(str(x).split()))\n",
    "df_w_topics['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply topic weight by word count\n",
    "df_w_topics['0_wc'] = df_w_topics[0] * df_w_topics['word_count']\n",
    "df_w_topics['0_wc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a for loop to do this for every topic\n",
    "col_list = []\n",
    "topic_columns = range(0,4)\n",
    "\n",
    "for num in topic_columns:\n",
    "    col = \"%d_wc\" % num\n",
    "    col_list.append(col)\n",
    "    df_w_topics[col] = df_w_topics[num] * df_w_topics['word_count']\n",
    "    \n",
    "df_w_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "- What is the total number of words aligned with each topic, by author gender?\n",
    "- What is the proportion of total words aligned with each topic, by author gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why might we want to do one calculation over the other? Take average topic weight per documents versus the average number of words aligned with each topic? What are the benefits/drawbacks of each method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "- Find the most prevalent topic in the corpus.\n",
    "- Find the least prevalent topic in the corpus.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "for e in col_list:\n",
    "    print(e)\n",
    "    print(df_w_topics[e].sum()/df_w_topics['word_count'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic prevalence over time <a id='time'></a>\n",
    "\n",
    "We can do the same as above, but by year, to graph the prevalence of each topic over time. Let's make some pretty subplots to display these trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_year = df_w_topics.groupby('year')\n",
    "fig3 = plt.figure()\n",
    "chrt = 0\n",
    "for e in col_list:\n",
    "    chrt += 1 \n",
    "    ax2 = fig3.add_subplot(2,3, chrt)\n",
    "    (grouped_year[e].sum()/grouped_year['word_count'].sum()).plot(\n",
    "        kind='line', title=e)\n",
    "    \n",
    "fig3.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 2 I interpret to be about battles in France. What is going on between 1880 and 1884 in France that might make this topic increasingly popular over this time period?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising topics with pyLDAvis <a id='viz'></a>\n",
    "\n",
    "Understanding the data that underlies a topic model is vital, but fortunately we also have a slightly more human-friendly option to help us interpret the topics!\n",
    "\n",
    "[pyLDAvis](https://github.com/bmabey/pyLDAvis) is a library for creating interactive topic model visualisations. It even has a helper function specifically for scikit-learn that we can use.\n",
    "\n",
    "> **It will take a while to load this visualisation!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Silence an annoying warning we cannot do anything about\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(tf_lda, tf_dtm, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some hints to help you interpret the visualisation:\n",
    "\n",
    "* On the **left-hand side** is a scatterplot of some bubbles:\n",
    " * Each **bubble** represents a topic.\n",
    " * The **size of a bubble** represents how _prevalent_ or popular the topic is overall.\n",
    " * The **distance** from one bubble to another represents how similar the topics are to each other. If they overlap then the topics share significant similarity.\n",
    " \n",
    "* On the **right-hand side** is a histogram of terms (tokens):\n",
    " * Select a bubble and it shows the top-30 **most relevant terms** for that topic.\n",
    " * The **red bar** represents how frequent a term is in the topic.\n",
    " * The **blue bar** represents how frequent the term is overall in all topics. So a long red bar with only a short blue bar indicates a term that is highly specific to that particular topic. Conversely, a red bar with a long blue bar means the term is also present in many other topics.\n",
    " * By mousing over a particular term, the size of the bubbles changes to show the relative frequency of that term in the various topics.\n",
    " * By adjusting the slide, it adjusts the **_relevance_ value (λ)**, which is the weight given to whether a term appears exclusively in a particular topic or is spread over topics more evenly. If λ = 1 terms are ranked according to their probabilities in the particular topic only; if λ = 0 terms are ranked higher if they are unusual terms that occur almost exclusively in that topic. Typically, the optimal value is around 0.6, but it is interesting to adjust it and observe any differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use `pyLDAvis` to visualize the relationships between topics in the `tfidf_lda_stemmed` model you created earlier. What differences do you notice between this and the un-stemmed version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the topic model <a id='eval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if our topic models are giving sensible results? One way to find out is by human coding and interpretation (e.g., expert annotation), which is slow and resource-intensive. Luckily, there are a number of quantitative approaches to evaluating the quality of topic models, some of which are built into `scikit-learn`, others of which we need to use outside packages. Let's look at these metrics.\n",
    "\n",
    "The _likelihood_ measures the probability of the observed data, given the model — i.e. how well a topic model fits the observed data. Similarly, _perplexity_ measures how well a model predicts a sample, i.e. how much it is “perplexed” by a sample from the observed data (a held-out test set). For both of these metrics, the lower the score, the better the model for the observed data. These are the main statistical measures used when constructing topic models&mdash;and even [when choosing the best one](https://datascience.blog.wzb.eu/2017/11/09/topic-modeling-evaluation-in-python-with-tmtoolkit/). However, neither one correlates very strongly with human evaluation of what topics make sense, i.e. what words should be together in which topics.\n",
    "\n",
    "To better reflect human judgment, a lot of work has been done to develop measures of topic model _coherence_, meaning the conditional likelihood of the co-occurrence of words in a topic. The higher the coherence, the more a topic model reflects human interpretation, i.e. expert annotation. In other words, coherence measures the degree of semantic similarity between high scoring words in a given topic, reflecting how semantically interpretable the topic is&mdash;as opposed to being an artifact of statistical inference. If you want to learn more about the math, intuition, and varieties of semantic coherence measures, check out [this blog](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0) and [this canonical article](https://dl.acm.org/doi/pdf/10.5555/2145432.2145462).\n",
    "\n",
    "First, let's look at likelihood (usually logged) and perplexity via `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Likelihood: Higher the better\n",
    "print(\"Log Likelihood: \", tf_lda.score(tf_dtm))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", tf_lda.perplexity(tf_dtm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check out coherence of our topic models, we will use the `tmtoolkit` module, which can calculate a number of different quantitative metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod import tm_sklearn \n",
    "tm_sklearn.AVAILABLE_METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the coherence using the formula proposed by [Mimno et al. (2011)](https://dl.acm.org/doi/pdf/10.5555/2145432.2145462):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_coherence_scores = tm_sklearn.metric_coherence_mimno_2011(\n",
    "    topic_word_distrib = tf_lda.components_,\n",
    "    dtm = tf_dtm)\n",
    "\n",
    "tf_coherence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Calculate the log-likelihood, perplexity, and coherence of the `tfidf_lda_stemmed` model you created earlier. How does this model compare to the 4-topic, un-stemmed one we just examined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Resources and alternatives <a id='resources'></a>\n",
    "\n",
    "In addition to LDA in `scikit-learn`, there are a few other common tools for topic modeling in Python:\n",
    "- [Here's a detailed example of using LDA in scikit-learn](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py), including several alternatives (like NMF) we won't explore.\n",
    "- The other major topic modeling package is [Gensim](https://radimrehurek.com/gensim/) (example implementation [here](https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb) and [here](https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb)). \n",
    "- Another option is [textacy](https://textacy.readthedocs.io/en/latest/), which is built on the powerful spaCy library for text manipulation ([example implementation](https://github.com/repmax/topic-model/blob/master/topic-modelling.ipynb)).\n",
    "\n",
    "Another well-known tool for topic modeling is called [MALLET](http://mallet.cs.umass.edu/topics.php), which is a program (written in Java) that you download to your computer. You have to type commands to use MALLET, but it has otherwise done a great deal for you. \n",
    "- [Getting Started with Topic Modeling and MALLET](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet) from Programming Historian gives a step-by-step tutorial on MALLET.\n",
    "- There is a graphical interface for MALLET called [Topic Modeling Tool](https://github.com/senderle/topic-modeling-tool) that is a bit easier to use. The [Quickstart Guide](https://senderle.github.io/topic-modeling-tool/documentation/2017/01/06/quickstart.html) will get you up and running.\n",
    "\n",
    "If you are looking to use R rather than Python, then `tidytext` is a popular NLP library that will help you work with the `topicmodels` package. \n",
    "- The book _Text Mining with R_ devotes [chapter 6](https://www.tidytextmining.com/topicmodeling.html) to tidytext.\n",
    "\n",
    "Finally, if coding isn't your thing, you can explore the topics of a few documents in a casual way with the online digital text environment [Voyant Tools](https://voyant-tools.org/), which allows you to upload or copy-and-paste texts and explore a corpus with a number of graphical tools, including topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
